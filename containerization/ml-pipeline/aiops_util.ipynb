{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63ca369f-036c-4748-a89d-8efea8cd37df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AIOps Util notebook containing common functions used in different notebooks and scripts.\n",
    "import json\n",
    "import adal\n",
    "import base64\n",
    "from pyspark.sql.functions import to_timestamp,lit\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from dateutil.parser import parse\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "47d7ada6-62cb-4df1-8eb8-1c56a667ad32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run ./icm_config.ipynb\n",
    "import os\n",
    "if os.environ['CUSTOMER'] == 'CSIO':\n",
    "    %run /home/jovyan/work/DAGS_Airflow/icm_config_csio.ipynb\n",
    "elif os.environ['CUSTOMER'] == 'globelife':\n",
    "    %run /home/jovyan/work/DAGS_Airflow/icm_config.ipynb\n",
    "elif os.environ['CUSTOMER'] == 'regeneron':\n",
    "    %run /home/jovyan/work/DAGS_Airflow/icm_config_regeneron.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f0a8035a-7852-4e11-a282-c463614b7bc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /home/jovyan/work/DAGS_Airflow/rca_config.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6af1c2aa-c6fd-401f-8d98-9dd25a0e8f24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Common Logging function for all the notebooks and scripts\n",
    "def get_logger(name, sc):\n",
    "  log4jLogger = sc._jvm.org.apache.log4j\n",
    "  logger = log4jLogger.LogManager.getLogger(name)\n",
    "  logger.info(\"pyspark script logger initialized for {}\".format(name))\n",
    "  \n",
    "  return logger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bde83543-a3c0-49a8-b2d6-a5a6b901ffef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "#logger = get_logger(\"AIOps_Util\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "53a70ddd-b239-490f-b79c-f283aee111ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This function is to read the data from azure blob storage. The function can either load all the files or only the latest file based on arguements passed.\n",
    "def load_blob_data_df(source_config, schema, last_run, load_only_latest_file):\n",
    "  \"\"\"\n",
    "  This function reads the data from blob to a spark dtaframe and returns it. Below are the input/output of the function.\n",
    "  Input: configurations, schema for the file, timestamp of the last read file, flag to read one file or all files\n",
    "  Output: dataframe, timestamp of the latest file\n",
    "  \"\"\"\n",
    "  # Load the blob credentials and Initialize\n",
    "  storage_account_name = source_config.storage_account_name\n",
    "  container_name = source_config.container_name\n",
    "  data_path = source_config.data_path\n",
    "  \n",
    "  # Converting string to timestamp format for last run from DB format to timestamp format \n",
    "  date_time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "  try:\n",
    "    last_run_timestamp = parse(last_run).strftime(date_time_format)\n",
    "  except:\n",
    "    last_run_timestamp = (datetime.datetime.now() - datetime.timedelta(days=30*365)).strftime(date_time_format) \n",
    "    \n",
    "  # Fetch blob credentials from dbutils secret store and generate the connection string \n",
    "  access_key = dbutils.secrets.get(scope = \"azure-blob-access-key\", key =\"access-token\")\n",
    "  azure_blob_base_path = 'wasbs://{}@{}.blob.core.windows.net/'.format(container_name, storage_account_name)\n",
    "  spark.conf.set(\"fs.azure.account.key.\"+storage_account_name+\".blob.core.windows.net\", access_key)\n",
    "  \n",
    "  try:\n",
    "    # Establish blob connection and load the data.\n",
    "    block_blob_service = BlockBlobService(account_name=storage_account_name, account_key=access_key)\n",
    "    generator = block_blob_service.list_blobs(container_name)\n",
    "    blob_df = spark.createDataFrame([], schema= schema)\n",
    "    latest_files = []\n",
    "    last_modified = last_run_timestamp\n",
    "    if not load_only_latest_file:\n",
    "      # This loads the the delta data files based on the last_modified file from blob storage \n",
    "      for blob in generator:\n",
    "        blob_last_modified_date = blob.properties.last_modified.strftime(date_time_format)\n",
    "        if blob_last_modified_date > last_run_timestamp and data_path in blob.name :\n",
    "          if(last_modified < blob_last_modified_date ):\n",
    "            last_modified = blob_last_modified_date\n",
    "          latest_files.append(azure_blob_base_path + blob.name)\n",
    "    else:\n",
    "      latest_file, last_modified = sorted([(blob.name, blob.properties.last_modified) for blob in generator], key = lambda x:x[1], reverse = True)[0]\n",
    "      latest_files.append(azure_blob_base_path + latest_file)\n",
    "\n",
    "  except:\n",
    "    return blob_df, last_modified\n",
    "  \n",
    "  # This DROPMALFORMED option while reading the data drops all the data rows which doesnt adhere to the configured schema.\n",
    "  if len(latest_files) > 0:\n",
    "    blob_df = spark.read.format(\"csv\")\\\n",
    "                   .schema(schema)\\\n",
    "                   .option(\"header\", \"True\")\\\n",
    "                   .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "                   .load(latest_files)\n",
    "  else:\n",
    "    return blob_df, last_modified\n",
    "  \n",
    "  return blob_df, last_modified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb52f42c-d97d-475d-adc2-ab168e9ed397",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data from the given database and database server based on the query supplied and return the data as a DataFrame.\n",
    "# This method uses the Azure Service Principle credentials for the database authentication.\n",
    "def load_data_df(db_server,port, db_name, query):\n",
    "  \n",
    "  logger.info(\"Performing data load from {} database\".format(db_name))\n",
    "  service_principal_id = dbutils.secrets.get(scope = \"aiops-secret-scope\", key =\"service_principal_id\")\n",
    "  service_principal_secret = dbutils.secrets.get(scope = \"aiops-secret-scope\", key =\"service_principal_secret\")\n",
    "  tenant_id = dbutils.secrets.get(scope = \"aiops-secret-scope\", key =\"tenant_id\")\n",
    "\n",
    "  # Form the JDBC url with the server name and database\n",
    "  db_url = \"jdbc:sqlserver://{}:{};database={};\".format(db_server,port, db_name)\n",
    "  \n",
    "  # Located in App Registrations from Azure Portal\n",
    "  authority = general_config['authority_url'] + tenant_id\n",
    "  context = adal.AuthenticationContext(authority)\n",
    "  resource_app_id_url = general_config['resource_app_url']\n",
    "  token = context.acquire_token_with_client_credentials(resource_app_id_url, service_principal_id, service_principal_secret)\n",
    "  access_token = token[\"accessToken\"]\n",
    "  \n",
    "  jdbc_df = spark.read.format(\"jdbc\") \\\n",
    "              .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "              .option(\"url\", db_url) \\\n",
    "              .option(\"encrypt\", \"true\") \\\n",
    "              .option(\"databaseName\", db_name) \\\n",
    "              .option(\"hostNameInCertificate\", \"*.database.windows.net\") \\\n",
    "              .option(\"query\", query) \\\n",
    "              .option(\"accessToken\", access_token) \\\n",
    "              .option(\"trustServerCertificate\" , \"true\") \\\n",
    "              .load() \n",
    "  \n",
    "  logger.info(\"Returning dataframe for data load from {}\".format(db_name))\n",
    "  # Return the final Dataframe\n",
    "  return jdbc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c547fcbc-627c-4173-b3ba-25f7c0376dd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data from the given ODS database and database server based on the query supplied and return the data as a DataFrame.\n",
    "# This method uses the Database user name and password for the database authentication.\n",
    "def load_data_df_ods(db_server,port, db_name, query):\n",
    "  \n",
    "  logger.info(\"Performing data load from {} database\".format(db_name))\n",
    "  ods_user_name = dbutils.secrets.get(scope = \"aiops-secret-scope\", key =\"ods_user_name\")\n",
    "  ods_user_password = dbutils.secrets.get(scope = \"aiops-secret-scope\", key =\"ods_user_password\")\n",
    "  \n",
    "  # Form the JDBC url with the server name and database\n",
    "  db_url = \"jdbc:sqlserver://{}:{};database={};\".format(db_server,port, db_name)\n",
    "  \n",
    "  jdbc_df=spark.read.format(\"jdbc\")\\\n",
    "        .option(\"driver\", 'com.microsoft.sqlserver.jdbc.SQLServerDriver')\\\n",
    "        .option(\"url\", db_url)\\\n",
    "        .option(\"query\",query)\\\n",
    "        .option(\"user\", ods_user_name)\\\n",
    "        .option(\"password\", ods_user_password)\\\n",
    "        .option(\"serverTimezone\", \"UTC\")\\\n",
    "        .load()\n",
    "\n",
    "  logger.info(\"Returning dataframe for data load from {}\".format(db_name))\n",
    "  # Return the final Dataframe\n",
    "  return jdbc_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9ad00c32-5d5a-4bfa-85ba-5738a043b63c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data from multiple query into a single dataframe\n",
    "def load_data_df_multi_query(db_server,port, db_name, *query):\n",
    "  df=load_data_df(db_server,port, db_name, query[0])\n",
    "  for i in range(1,len(query)): \n",
    "      df=df.join(load_data_df(db_server,port, db_name, query[i]), on=['CHECK_TIME', 'DEVICE'], how='outer')\n",
    "  logger.info(\"Returning combined dataframe for data load from {}\".format(db_name))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0d6cbc5f-27b8-44c4-a80b-1a69da35c24a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the supplied dataframe into Modeling DB\n",
    "def write_df(df,table_name,option):\n",
    "  \n",
    "  logger.info(\"Started persisting the dataframe into {} in Modeling DB\".format(table_name))\n",
    "  db_server=icm_modelDBconfig['db_server_name']\n",
    "  db_port=icm_modelDBconfig['db_port']\n",
    "  db_name=icm_modelDBconfig['database_name']\n",
    "  \n",
    "  service_principal_id = dbutils.secrets.get(scope = \"aiops-secret-scope\", key =\"service_principal_id\")\n",
    "  service_principal_secret = dbutils.secrets.get(scope = \"aiops-secret-scope\", key =\"service_principal_secret\")\n",
    "  tenant_id = dbutils.secrets.get(scope = \"aiops-secret-scope\", key =\"tenant_id\")\n",
    "\n",
    "  # Located in App Registrations from Azure Portal\n",
    "  authority = general_config['authority_url'] + tenant_id\n",
    "  context = adal.AuthenticationContext(authority)\n",
    "  resource_app_id_url = general_config['resource_app_url']\n",
    "  token = context.acquire_token_with_client_credentials(resource_app_id_url, service_principal_id, service_principal_secret)\n",
    "  access_token = token[\"accessToken\"]\n",
    "    \n",
    "  url = \"jdbc:sqlserver://{}:{};\".format(db_server,db_port)\n",
    "  connectionProperties = {\n",
    "    \"database\":db_name,\n",
    "    \"driver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "    \"accessToken\":access_token,\n",
    "    \"encrypt\":\"true\",\n",
    "    \"hostNameInCertificate\":\"*.database.windows.net\"\n",
    "  }\n",
    "  \n",
    "\n",
    "  # Write is put under try catch in case overwrite to avoid data loss and store in backup table\n",
    "  if(option=='overwrite'):  \n",
    "    try:\n",
    "      df.write.option(\"truncate\", \"true\").jdbc(url, table_name, option, connectionProperties)\n",
    "      logger.info(\"Successfully persisted dataframe into {} in Modeling DB\".format(table_name))\n",
    "    except:\n",
    "      created_on = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "      df=df.withColumn('exception_on',lit(created_on).cast(StringType()))\n",
    "      \n",
    "      backup_file_location= '/backup_files/'\n",
    "      # creating directory if not present\n",
    "      dbutils.fs.mkdirs(backup_file_location)\n",
    "      filepath = '/dbfs'+backup_file_location+table_name+'_'+created_on+'.csv'\n",
    "      # Storing the data to excel file if any issues comes during updating the data to DB\n",
    "      df.toPandas().to_csv(filepath)\n",
    "\n",
    "      df.write.option(\"truncate\", \"true\").jdbc(url, table_name+'_backup', 'append', connectionProperties)\n",
    "      logger.info(\"Error while overwriting the {} table\".format(table_name))\n",
    "  else:\n",
    "    df.write.option(\"truncate\", \"true\").jdbc(url, table_name, option, connectionProperties)\n",
    "    logger.info(\"Successfully persisted dataframe into {} in Modeling DB\".format(table_name))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8c6180d8-e281-48b4-b101-6293c0ee9af7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Module will return Custom or generic cost sheet name w.r.t each customer\n",
    "def get_cost_sheet_name(customer,cloud_name):\n",
    "  custom_sheet = customer + '_'+ cloud_name + '_pricing'\n",
    "  dbfs_table_df = spark.sql('show tables')\n",
    "  df = dbfs_table_df.select('tableName').filter((dbfs_table_df['tableName'] == custom_sheet))\n",
    "  if df.count()>0:\n",
    "    cost_sheet_name = custom_sheet\n",
    "  else:\n",
    "    cost_sheet_name = cloud_name + '_pricing_csv'\n",
    "  return cost_sheet_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4aefd51-0e7c-4667-a25f-99713890933f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_data_to_db(pandas_df,table,write_type):\n",
    "  ''' This function takes the pandas dataframe and converts it into spark dataframe and writes it into DB with exception handling  \n",
    "    Parameters:\n",
    "      a : pandas_df - Pandas Dataframe \n",
    "      b : table - DB table name\n",
    "      c): write_type - Option to append or overwrite the table  '''   \n",
    "  try:\n",
    "    if not pandas_df.empty:\n",
    "      pandas_df = pandas_df.fillna(value=pd.np.nan)\n",
    "      spark_df = spark.createDataFrame(pandas_df)\n",
    "      spark_df = spark_df.replace(float('nan'), None)\n",
    "      write_df(spark_df , table, write_type)\n",
    "      logger.info(table + \" - Table stored to database\")   \n",
    "    else:\n",
    "      logger.info(table + \" - No Data\")\n",
    "  except:\n",
    "      logger.info(table + \" - Error writing data to database\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "aiops_util",
   "notebookOrigID": 2655774084279600,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
